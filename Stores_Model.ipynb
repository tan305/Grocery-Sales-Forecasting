{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import datetime\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, Input, LSTM\n",
    "from keras.models import Sequential, Model,load_model\n",
    "from keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Input, Convolution1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch.nn as nn\n",
    "import datetime\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_agg_data=pd.read_csv(\"stores_agg.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_agg_data.drop(\"transactions\",inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = torch.tensor(final_agg_data['unit_sales'].values.astype(np.float32))\n",
    "X1 = torch.tensor(final_agg_data.drop('unit_sales', axis = 1).values.astype(np.float32)) \n",
    "dataset1 = TensorDataset(X1,y1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set1, val_set1 = torch.utils.data.random_split(dataset1, [66488, 16622],)\n",
    "train_loader1 = DataLoader(dataset = train_set1, shuffle = True,batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_loader1=DataLoader(dataset = val_set1, shuffle = True,batch_size=16622)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in validation_loader1:\n",
    "    X_test1,y_test1=i,j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = torch.tensor(final_agg_data['unit_sales'].values.astype(np.float32))[:4500]\n",
    "X1 = torch.tensor(final_agg_data.drop('unit_sales', axis = 1).values.astype(np.float32))[:4500] \n",
    "dataset1 = TensorDataset(X1,y1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set1, val_set1 = torch.utils.data.random_split(dataset1, [4000,500])\n",
    "train_loader1 = DataLoader(dataset = train_set1, shuffle = True,batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_loader1=DataLoader(dataset = val_set1, shuffle = True,batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in validation_loader1:\n",
    "    X_test1,y_test1=i,j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FirstNetwork_v1(nn.Module):\n",
    "    def __init__(self,number_of_nodes):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(83,number_of_nodes)\n",
    "        self.lin2 = nn.Linear(number_of_nodes, 1)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        a1 = self.lin1(X)\n",
    "        h1 = a1.relu()\n",
    "        a2 = self.lin2(h1)\n",
    "        return a2,a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models_train_loss={}\n",
    "all_models_val_loss={}\n",
    "all_models_time={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Model with 15  nodes\n",
      "0 Times Model Repeated\n",
      "1 Times Model Repeated\n",
      "2 Times Model Repeated\n",
      "3 Times Model Repeated\n",
      "4 Times Model Repeated\n",
      "5 Times Model Repeated\n",
      "6 Times Model Repeated\n",
      "7 Times Model Repeated\n",
      "8 Times Model Repeated\n",
      "9 Times Model Repeated\n",
      "Running Model with 20  nodes\n",
      "0 Times Model Repeated\n",
      "1 Times Model Repeated\n",
      "2 Times Model Repeated\n",
      "3 Times Model Repeated\n",
      "4 Times Model Repeated\n",
      "5 Times Model Repeated\n",
      "6 Times Model Repeated\n",
      "7 Times Model Repeated\n",
      "8 Times Model Repeated\n",
      "9 Times Model Repeated\n",
      "Running Model with 25  nodes\n",
      "0 Times Model Repeated\n",
      "1 Times Model Repeated\n",
      "2 Times Model Repeated\n",
      "3 Times Model Repeated\n",
      "4 Times Model Repeated\n",
      "5 Times Model Repeated\n",
      "6 Times Model Repeated\n",
      "7 Times Model Repeated\n",
      "8 Times Model Repeated\n",
      "9 Times Model Repeated\n",
      "Running Model with 30  nodes\n",
      "0 Times Model Repeated\n",
      "1 Times Model Repeated\n",
      "2 Times Model Repeated\n",
      "3 Times Model Repeated\n",
      "4 Times Model Repeated\n",
      "5 Times Model Repeated\n",
      "6 Times Model Repeated\n",
      "7 Times Model Repeated\n",
      "8 Times Model Repeated\n",
      "9 Times Model Repeated\n",
      "Running Model with 35  nodes\n",
      "0 Times Model Repeated\n",
      "1 Times Model Repeated\n",
      "2 Times Model Repeated\n",
      "3 Times Model Repeated\n",
      "4 Times Model Repeated\n",
      "5 Times Model Repeated\n",
      "6 Times Model Repeated\n",
      "7 Times Model Repeated\n",
      "8 Times Model Repeated\n",
      "9 Times Model Repeated\n",
      "Running Model with 40  nodes\n",
      "0 Times Model Repeated\n",
      "1 Times Model Repeated\n",
      "2 Times Model Repeated\n",
      "3 Times Model Repeated\n",
      "4 Times Model Repeated\n",
      "5 Times Model Repeated\n",
      "6 Times Model Repeated\n",
      "7 Times Model Repeated\n",
      "8 Times Model Repeated\n",
      "9 Times Model Repeated\n"
     ]
    }
   ],
   "source": [
    "for i in range(15,41,5):\n",
    "    print(\"Running Model with \"+str(i),\" nodes\")\n",
    "    all_models_train_loss[i]=[]\n",
    "    all_models_val_loss[i]=[] \n",
    "    all_models_time[i]=[]\n",
    "    for p in range(10):\n",
    "#         torch.manual_seed(np.random.randint(0,1000000,1))\n",
    "#         train_set1, val_set1 = torch.utils.data.random_split(dataset1, [4000,500])\n",
    "#         train_loader1 = DataLoader(dataset = train_set1, shuffle = True,batch_size=16)\n",
    "#         validation_loader1=DataLoader(dataset = val_set1, shuffle = True,batch_size=500)\n",
    "        for c,v in validation_loader1:\n",
    "            X_test1,y_test1=c,v\n",
    "        print(str(p)+\" Times Model Repeated\")\n",
    "        start = torch.cuda.Event(enable_timing=True)\n",
    "        end = torch.cuda.Event(enable_timing=True)\n",
    "        model1=FirstNetwork_v1(number_of_nodes=i).to(device)\n",
    "        criterion1 = nn.MSELoss()\n",
    "        training_loss_model1=[]\n",
    "        validation_loss_model1=[]\n",
    "        time_model1=[]\n",
    "        epochs = 100\n",
    "        optimizer1 = optim.SGD(model1.parameters(), lr=0.01)\n",
    "        for epoch in range(epochs):\n",
    "            training_loss_batch=[]\n",
    "            validation_loss_batch=[]\n",
    "            time_batch=[]\n",
    "            for X_train, y_train in train_loader1:\n",
    "                model1.train()\n",
    "                optimizer1.zero_grad()\n",
    "                X_train,y_train=X_train.to(device),y_train.to(device)\n",
    "                start.record()\n",
    "                output,features = model1(X_train)\n",
    "                loss1 = criterion1(output, y_train.reshape(-1,1))\n",
    "                loss1.backward()\n",
    "                optimizer1.step()\n",
    "                end.record()\n",
    "                training_loss_batch.append(loss1.item())\n",
    "                model1.eval()\n",
    "                validation_loss_batch.append(criterion1(model1(X_test1.to(device))[0],y_test1.to(device).reshape(-1,1)).item())\n",
    "                time_batch.append(start.elapsed_time(end))\n",
    "            training_loss_model1.append(training_loss_batch)\n",
    "            validation_loss_model1.append(validation_loss_batch)\n",
    "            time_model1.append(time_batch)\n",
    "        del model1\n",
    "        torch.cuda.empty_cache()\n",
    "        all_models_val_loss[i].append(validation_loss_model1)\n",
    "        all_models_train_loss[i].append(training_loss_model1)\n",
    "        all_models_time[i].append(time_model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('store_model_train_sub.p', 'wb') as fp:\n",
    "    pickle.dump(all_models_train_loss, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('store_model_val_sub.p', 'wb') as fp:\n",
    "    pickle.dump(all_models_val_loss, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('store_model_time_sub.p', 'wb') as fp:\n",
    "    pickle.dump(all_models_time, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 0.0006706327103311197 8.873485486296187e-05\n",
      "20 0.0005945751896593719 3.882718091821089e-05\n",
      "25 0.0006343823679955676 6.044432135587443e-05\n",
      "30 0.0005887161928927525 0.000102450483857153\n",
      "35 0.0005606920357095077 3.426764732307398e-05\n",
      "40 0.0005674828005954623 4.643754637914265e-05\n"
     ]
    }
   ],
   "source": [
    "for m in sorted(all_models_val_loss.keys()):\n",
    "    print(m,np.mean([min([np.mean(all_models_val_loss[m][k][i]) for i in range(len(all_models_val_loss[m][k]))]) for k in range(len(all_models_val_loss[m]))]),\n",
    "         np.std([min([np.mean(all_models_val_loss[m][k][i]) for i in range(len(all_models_val_loss[m][k]))]) for k in range(len(all_models_val_loss[m]))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models_train_loss={}\n",
    "all_models_val_loss={}\n",
    "all_models_time={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Model with 25  nodes\n",
      "0 Times Model Repeated\n",
      "1 Times Model Repeated\n",
      "2 Times Model Repeated\n",
      "3 Times Model Repeated\n",
      "4 Times Model Repeated\n",
      "5 Times Model Repeated\n",
      "6 Times Model Repeated\n",
      "7 Times Model Repeated\n",
      "8 Times Model Repeated\n",
      "9 Times Model Repeated\n"
     ]
    }
   ],
   "source": [
    "for i in range(25,26,5):\n",
    "    print(\"Running Model with \"+str(i),\" nodes\")\n",
    "    all_models_train_loss[i]=[]\n",
    "    all_models_val_loss[i]=[] \n",
    "    all_models_time[i]=[]\n",
    "    for p in range(10):\n",
    "        #torch.manual_seed(np.random.randint(0,1000000,1))\n",
    "#         train_set1, val_set1 = torch.utils.data.random_split(dataset1, [4000,500])\n",
    "#         train_loader1 = DataLoader(dataset = train_set1, shuffle = True,batch_size=16)\n",
    "#         validation_loader1=DataLoader(dataset = val_set1, shuffle = True,batch_size=500)\n",
    "        for c,v in validation_loader1:\n",
    "            X_test1,y_test1=c,v\n",
    "        print(str(p)+\" Times Model Repeated\")\n",
    "        start = torch.cuda.Event(enable_timing=True)\n",
    "        end = torch.cuda.Event(enable_timing=True)\n",
    "        model1=FirstNetwork_v1(number_of_nodes=i).to(device)\n",
    "        criterion1 = nn.MSELoss()\n",
    "        training_loss_model1=[]\n",
    "        validation_loss_model1=[]\n",
    "        time_model1=[]\n",
    "        epochs = 100\n",
    "        optimizer1 = optim.Adam(model1.parameters(), lr=0.01)\n",
    "        for epoch in range(epochs):\n",
    "            training_loss_batch=[]\n",
    "            validation_loss_batch=[]\n",
    "            time_batch=[]\n",
    "            for X_train, y_train in train_loader1:\n",
    "                model1.train()\n",
    "                optimizer1.zero_grad()\n",
    "                X_train,y_train=X_train.to(device),y_train.to(device)\n",
    "                start.record()\n",
    "                output,features = model1(X_train)\n",
    "                loss1 = criterion1(output, y_train.reshape(-1,1))\n",
    "                loss1.backward()\n",
    "                optimizer1.step()\n",
    "                end.record()\n",
    "                training_loss_batch.append(loss1.item())\n",
    "                model1.eval()\n",
    "                validation_loss_batch.append(criterion1(model1(X_test1.to(device))[0],y_test1.to(device).reshape(-1,1)).item())\n",
    "                time_batch.append(start.elapsed_time(end))\n",
    "            training_loss_model1.append(training_loss_batch)\n",
    "            validation_loss_model1.append(validation_loss_batch)\n",
    "            time_model1.append(time_batch)\n",
    "        del model1\n",
    "        torch.cuda.empty_cache()\n",
    "        all_models_val_loss[i].append(validation_loss_model1)\n",
    "        all_models_train_loss[i].append(training_loss_model1)\n",
    "        all_models_time[i].append(time_model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('store_model_train_sub1.p', 'wb') as fp:\n",
    "    pickle.dump(all_models_train_loss, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('store_model_val_sub1.p', 'wb') as fp:\n",
    "    pickle.dump(all_models_val_loss, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('store_model_time_sub1.p', 'wb') as fp:\n",
    "    pickle.dump(all_models_time, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_list=[]\n",
    "for y in range(10):\n",
    "    min_arg=np.argmin([np.mean(all_models_time[25][y][i]) for i in range(len(all_models_time[25][y]))])\n",
    "    time_list.append(np.cumsum([sum(all_models_time[25][y][i]) for i in range(len(all_models_time[25][y]))][:min_arg])[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29500.5981136322"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(time_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 0.00034839822872309014 1.1648447180425292e-05\n"
     ]
    }
   ],
   "source": [
    "for m in sorted(all_models_val_loss.keys()):\n",
    "    print(m,np.mean([min([np.mean(all_models_val_loss[m][k][i]) for i in range(len(all_models_val_loss[m][k]))]) for k in range(len(all_models_val_loss[m]))]),\n",
    "         np.std([min([np.mean(all_models_val_loss[m][k][i]) for i in range(len(all_models_val_loss[m][k]))]) for k in range(len(all_models_val_loss[m]))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = torch.tensor(final_agg_data['unit_sales'].values.astype(np.float32))[:4500]\n",
    "X1 = torch.tensor(final_agg_data.drop('unit_sales', axis = 1).values.astype(np.float32))[:4500]\n",
    "dataset1 = TensorDataset(X1,y1) \n",
    "\n",
    "train_set1, val_set1 = torch.utils.data.random_split(dataset1, [4000, 500],)\n",
    "train_loader1 = DataLoader(dataset = train_set1, shuffle = True,batch_size=32)\n",
    "\n",
    "validation_loader1=DataLoader(dataset = val_set1, shuffle = True,batch_size=500)\n",
    "\n",
    "for i,j in validation_loader1:\n",
    "    X_test1,y_test1=i,j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Model with 20  nodes\n",
      "0 Times Model Repeated\n",
      "1 Times Model Repeated\n",
      "2 Times Model Repeated\n",
      "3 Times Model Repeated\n",
      "4 Times Model Repeated\n",
      "5 Times Model Repeated\n",
      "6 Times Model Repeated\n",
      "7 Times Model Repeated\n",
      "8 Times Model Repeated\n",
      "9 Times Model Repeated\n",
      "Running Model with 25  nodes\n",
      "0 Times Model Repeated\n",
      "1 Times Model Repeated\n",
      "2 Times Model Repeated\n",
      "3 Times Model Repeated\n",
      "4 Times Model Repeated\n",
      "5 Times Model Repeated\n",
      "6 Times Model Repeated\n",
      "7 Times Model Repeated\n",
      "8 Times Model Repeated\n",
      "9 Times Model Repeated\n",
      "Running Model with 30  nodes\n",
      "0 Times Model Repeated\n",
      "1 Times Model Repeated\n",
      "2 Times Model Repeated\n",
      "3 Times Model Repeated\n",
      "4 Times Model Repeated\n",
      "5 Times Model Repeated\n",
      "6 Times Model Repeated\n",
      "7 Times Model Repeated\n",
      "8 Times Model Repeated\n",
      "9 Times Model Repeated\n",
      "Running Model with 35  nodes\n",
      "0 Times Model Repeated\n",
      "1 Times Model Repeated\n",
      "2 Times Model Repeated\n",
      "3 Times Model Repeated\n",
      "4 Times Model Repeated\n",
      "5 Times Model Repeated\n",
      "6 Times Model Repeated\n",
      "7 Times Model Repeated\n",
      "8 Times Model Repeated\n",
      "9 Times Model Repeated\n",
      "Running Model with 40  nodes\n",
      "0 Times Model Repeated\n",
      "1 Times Model Repeated\n",
      "2 Times Model Repeated\n",
      "3 Times Model Repeated\n",
      "4 Times Model Repeated\n",
      "5 Times Model Repeated\n",
      "6 Times Model Repeated\n",
      "7 Times Model Repeated\n",
      "8 Times Model Repeated\n",
      "9 Times Model Repeated\n"
     ]
    }
   ],
   "source": [
    "all_models_train_loss={}\n",
    "all_models_val_loss={}\n",
    "all_models_time={}\n",
    "\n",
    "for i in range(20,41,5):\n",
    "    print(\"Running Model with \"+str(i),\" nodes\")\n",
    "    all_models_train_loss[i]=[]\n",
    "    all_models_val_loss[i]=[] \n",
    "    all_models_time[i]=[]\n",
    "    for p in range(10):\n",
    "        #torch.manual_seed(np.random.randint(0,1000000,1))\n",
    "#         train_set1, val_set1 = torch.utils.data.random_split(dataset1, [4000,500])\n",
    "#         train_loader1 = DataLoader(dataset = train_set1, shuffle = True,batch_size=16)\n",
    "#         validation_loader1=DataLoader(dataset = val_set1, shuffle = True,batch_size=500)\n",
    "        for c,v in validation_loader1:\n",
    "            X_test1,y_test1=c,v\n",
    "        print(str(p)+\" Times Model Repeated\")\n",
    "        start = torch.cuda.Event(enable_timing=True)\n",
    "        end = torch.cuda.Event(enable_timing=True)\n",
    "        model1=FirstNetwork_v1(number_of_nodes=i).to(device)\n",
    "        criterion1 = nn.MSELoss()\n",
    "        training_loss_model1=[]\n",
    "        validation_loss_model1=[]\n",
    "        time_model1=[]\n",
    "        epochs = 100\n",
    "        optimizer1 = optim.Adam(model1.parameters(), lr=0.01)\n",
    "        for epoch in range(epochs):\n",
    "            training_loss_batch=[]\n",
    "            validation_loss_batch=[]\n",
    "            time_batch=[]\n",
    "            for X_train, y_train in train_loader1:\n",
    "                model1.train()\n",
    "                optimizer1.zero_grad()\n",
    "                X_train,y_train=X_train.to(device),y_train.to(device)\n",
    "                start.record()\n",
    "                output,features = model1(X_train)\n",
    "                loss1 = criterion1(output, y_train.reshape(-1,1))\n",
    "                loss1.backward()\n",
    "                optimizer1.step()\n",
    "                end.record()\n",
    "                training_loss_batch.append(loss1.item())\n",
    "                model1.eval()\n",
    "                validation_loss_batch.append(criterion1(model1(X_test1.to(device))[0],y_test1.to(device).reshape(-1,1)).item())\n",
    "                time_batch.append(start.elapsed_time(end))\n",
    "            training_loss_model1.append(training_loss_batch)\n",
    "            validation_loss_model1.append(validation_loss_batch)\n",
    "            time_model1.append(time_batch)\n",
    "        del model1\n",
    "        torch.cuda.empty_cache()\n",
    "        all_models_val_loss[i].append(validation_loss_model1)\n",
    "        all_models_train_loss[i].append(training_loss_model1)\n",
    "        all_models_time[i].append(time_model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('store_model_train_sub2.p', 'wb') as fp:\n",
    "    pickle.dump(all_models_train_loss, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('store_model_val_sub2.p', 'wb') as fp:\n",
    "    pickle.dump(all_models_val_loss, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('store_model_time_sub2.p', 'wb') as fp:\n",
    "    pickle.dump(all_models_time, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 0.0004994891935959459 3.117174955118241e-05\n",
      "25 0.0004734137291554362 3.952852312891622e-05\n",
      "30 0.0004859088379424065 5.090764519826238e-05\n",
      "35 0.0004922704371158034 3.380633383850725e-05\n",
      "40 0.0004949642788851633 2.6035526032642234e-05\n"
     ]
    }
   ],
   "source": [
    "for m in sorted(all_models_val_loss.keys()):\n",
    "    print(m,np.mean([min([np.mean(all_models_val_loss[m][k][i]) for i in range(len(all_models_val_loss[m][k]))]) for k in range(len(all_models_val_loss[m]))]),\n",
    "         np.std([min([np.mean(all_models_val_loss[m][k][i]) for i in range(len(all_models_val_loss[m][k]))]) for k in range(len(all_models_val_loss[m]))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"store_model_val_sub.p\", \"rb\") as input_file:\n",
    "    all_models_val_loss= pickle.load(input_file)\n",
    "\n",
    "item_model_loss={}\n",
    "\n",
    "for m in sorted(all_models_val_loss.keys()):\n",
    "        item_model_loss[m]=[min([np.mean(all_models_val_loss[m][k][i]) for i in range(len(all_models_val_loss[m][k]))]) for k in range(len(all_models_val_loss[m]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"store_model_val_sub1.p\", \"rb\") as input_file:\n",
    "    all_models_val_loss= pickle.load(input_file)\n",
    "\n",
    "item_model_loss1={}\n",
    "\n",
    "for m in sorted(all_models_val_loss.keys()):\n",
    "        item_model_loss1[m]=[min([np.mean(all_models_val_loss[m][k][i]) for i in range(len(all_models_val_loss[m][k]))]) for k in range(len(all_models_val_loss[m]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"store_model_val_sub2.p\", \"rb\") as input_file:\n",
    "    all_models_val_loss= pickle.load(input_file)\n",
    "\n",
    "item_model_loss2={}\n",
    "\n",
    "for m in sorted(all_models_val_loss.keys()):\n",
    "        item_model_loss2[m]=[min([np.mean(all_models_val_loss[m][k][i]) for i in range(len(all_models_val_loss[m][k]))]) for k in range(len(all_models_val_loss[m]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1dab0e31710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,3,figsize=(20,8),sharey=True)\n",
    "ax[0].boxplot(item_model_loss.values(),labels=item_model_loss.keys())\n",
    "ax[1].boxplot(item_model_loss1.values(),labels=item_model_loss1.keys())\n",
    "ax[2].boxplot(item_model_loss2.values(),labels=item_model_loss2.keys());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"store_model_time_sub2.p\", \"rb\") as input_file:\n",
    "    all_models_val_loss= pickle.load(input_file)\n",
    "\n",
    "item_model_loss1={}\n",
    "\n",
    "for m in sorted(all_models_val_loss.keys()):\n",
    "        item_model_loss1[m]=[min([np.mean(all_models_val_loss[m][k][i]) for i in range(len(all_models_val_loss[m][k]))]) for k in range(len(all_models_val_loss[m]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.910198522567749,\n",
       " 2.922741502761841,\n",
       " 2.88907928276062,\n",
       " 2.9271982135772707,\n",
       " 2.9103477725982665,\n",
       " 2.9183098773956297,\n",
       " 2.916229621887207,\n",
       " 2.9138618698120116,\n",
       " 2.9096596508026122,\n",
       " 2.893749755859375]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_model_loss1[25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = torch.tensor(final_agg_data['unit_sales'].values.astype(np.float32))\n",
    "X1 = torch.tensor(final_agg_data.drop('unit_sales', axis = 1).values.astype(np.float32)) \n",
    "dataset1 = TensorDataset(X1,y1) \n",
    "\n",
    "train_set1, val_set1 = torch.utils.data.random_split(dataset1, [66488, 16622],)\n",
    "train_loader1 = DataLoader(dataset = train_set1, shuffle = True,batch_size=16)\n",
    "\n",
    "validation_loader1=DataLoader(dataset = val_set1, shuffle = True,batch_size=16622)\n",
    "\n",
    "for i,j in validation_loader1:\n",
    "    X_test1,y_test1=i,j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FirstNetwork_final(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(0)\n",
    "        self.lin1 = nn.Linear(83,25)\n",
    "        self.lin2 = nn.Linear(25, 1)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        a1 = self.lin1(X)\n",
    "        h1 = a1.relu()\n",
    "        a2 = self.lin2(h1)\n",
    "        return a2,a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1=FirstNetwork_final()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Training Loss: 0.0005937622772298613\n",
      "Validation Loss: 0.0005488837452260155\n",
      "10\n",
      "Training Loss: 0.0005307834128015814\n",
      "Validation Loss: 0.00048798109912225003\n",
      "20\n",
      "Training Loss: 0.000539958095696884\n",
      "Validation Loss: 0.0004973362102104361\n",
      "30\n",
      "Training Loss: 0.0005358810311926084\n",
      "Validation Loss: 0.0004984814802878712\n"
     ]
    }
   ],
   "source": [
    "criterion1 = nn.MSELoss()\n",
    "\n",
    "training_loss_model1=[]\n",
    "validation_loss_model1=[]\n",
    "\n",
    "epochs = 40\n",
    "optimizer1 = optim.Adam(model1.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    training_loss_batch=[]\n",
    "    validation_loss_batch=[]\n",
    "    for X_train, y_train in train_loader1:\n",
    "        model1.train()\n",
    "        optimizer1.zero_grad()\n",
    "        output,features = model1(X_train)\n",
    "        loss1 = criterion1(output, y_train.reshape(-1,1))\n",
    "        loss1.backward()\n",
    "        optimizer1.step()\n",
    "        training_loss_batch.append(loss1.item())\n",
    "        model1.eval()\n",
    "        validation_loss_batch.append(criterion1(model1(X_test1)[0],y_test1.reshape(-1,1)).item())\n",
    "    validation_loss_model1.append(validation_loss_batch)\n",
    "    training_loss_model1.append(training_loss_batch)\n",
    "    if epoch%10==0:\n",
    "        print(epoch)\n",
    "        print(\"Training Loss:\",np.mean(training_loss_batch))\n",
    "        print(\"Validation Loss:\",np.mean(validation_loss_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "Training Loss: 0.00041670804667275985\n",
      "Validation Loss: 0.0003705873554092348\n",
      "50\n",
      "Training Loss: 0.0004103782580456934\n",
      "Validation Loss: 0.00036635024500830383\n",
      "60\n",
      "Training Loss: 0.00040948702737490447\n",
      "Validation Loss: 0.00036463019897838825\n",
      "70\n",
      "Training Loss: 0.00040972938838813405\n",
      "Validation Loss: 0.0003653913554059287\n",
      "80\n",
      "Training Loss: 0.0004111796339050244\n",
      "Validation Loss: 0.0003662864031861523\n",
      "90\n",
      "Training Loss: 0.0004099144978863653\n",
      "Validation Loss: 0.00036555995756525615\n",
      "100\n",
      "Training Loss: 0.00040946170592982985\n",
      "Validation Loss: 0.0003645406403645821\n",
      "110\n",
      "Training Loss: 0.00041041027444627234\n",
      "Validation Loss: 0.0003641856148307565\n",
      "120\n",
      "Training Loss: 0.0004100396466560137\n",
      "Validation Loss: 0.0003651991240735456\n",
      "130\n",
      "Training Loss: 0.00041011266612442447\n",
      "Validation Loss: 0.00036500573345492267\n"
     ]
    }
   ],
   "source": [
    "optimizer1 = optim.Adam(model1.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(40,140):\n",
    "    training_loss_batch=[]\n",
    "    validation_loss_batch=[]\n",
    "    for X_train, y_train in train_loader1:\n",
    "        model1.train()\n",
    "        optimizer1.zero_grad()\n",
    "        output,features = model1(X_train)\n",
    "        loss1 = criterion1(output, y_train.reshape(-1,1))\n",
    "        loss1.backward()\n",
    "        optimizer1.step()\n",
    "        training_loss_batch.append(loss1.item())\n",
    "        model1.eval()\n",
    "        validation_loss_batch.append(criterion1(model1(X_test1)[0],y_test1.reshape(-1,1)).item())\n",
    "    validation_loss_model1.append(validation_loss_batch)\n",
    "    training_loss_model1.append(training_loss_batch)\n",
    "    if epoch%10==0:\n",
    "        print(epoch)\n",
    "        print(\"Training Loss:\",np.mean(training_loss_batch))\n",
    "        print(\"Validation Loss:\",np.mean(validation_loss_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140\n",
      "Training Loss: 0.00039874573577007463\n",
      "Validation Loss: 0.0003546733622949237\n",
      "150\n",
      "Training Loss: 0.0003979502398688335\n",
      "Validation Loss: 0.00035380173512897836\n",
      "160\n",
      "Training Loss: 0.0003981540647385148\n",
      "Validation Loss: 0.0003535072512838284\n",
      "170\n",
      "Training Loss: 0.00039835251999787065\n",
      "Validation Loss: 0.0003533483893068012\n"
     ]
    }
   ],
   "source": [
    "optimizer1 = optim.Adam(model1.parameters(), lr=0.0001)\n",
    "\n",
    "for epoch in range(140,180):\n",
    "    training_loss_batch=[]\n",
    "    validation_loss_batch=[]\n",
    "    for X_train, y_train in train_loader1:\n",
    "        model1.train()\n",
    "        optimizer1.zero_grad()\n",
    "        output,features = model1(X_train)\n",
    "        loss1 = criterion1(output, y_train.reshape(-1,1))\n",
    "        loss1.backward()\n",
    "        optimizer1.step()\n",
    "        training_loss_batch.append(loss1.item())\n",
    "        model1.eval()\n",
    "        validation_loss_batch.append(criterion1(model1(X_test1)[0],y_test1.reshape(-1,1)).item())\n",
    "    validation_loss_model1.append(validation_loss_batch)\n",
    "    training_loss_model1.append(training_loss_batch)\n",
    "    if epoch%10==0:\n",
    "        print(epoch)\n",
    "        print(\"Training Loss:\",np.mean(training_loss_batch))\n",
    "        print(\"Validation Loss:\",np.mean(validation_loss_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180\n",
      "Training Loss: 0.00039700646394610163\n",
      "Validation Loss: 0.0003532861326069883\n",
      "190\n",
      "Training Loss: 0.0003967019753357882\n",
      "Validation Loss: 0.0003525075548128122\n",
      "200\n",
      "Training Loss: 0.0003966664006715557\n",
      "Validation Loss: 0.000352504033200307\n"
     ]
    }
   ],
   "source": [
    "optimizer1 = optim.Adam(model1.parameters(), lr=0.00001)\n",
    "\n",
    "for epoch in range(180,210):\n",
    "    training_loss_batch=[]\n",
    "    validation_loss_batch=[]\n",
    "    for X_train, y_train in train_loader1:\n",
    "        model1.train()\n",
    "        optimizer1.zero_grad()\n",
    "        output,features = model1(X_train)\n",
    "        loss1 = criterion1(output, y_train.reshape(-1,1))\n",
    "        loss1.backward()\n",
    "        optimizer1.step()\n",
    "        training_loss_batch.append(loss1.item())\n",
    "        model1.eval()\n",
    "        validation_loss_batch.append(criterion1(model1(X_test1)[0],y_test1.reshape(-1,1)).item())\n",
    "    validation_loss_model1.append(validation_loss_batch)\n",
    "    training_loss_model1.append(training_loss_batch)\n",
    "    if epoch%10==0:\n",
    "        print(epoch)\n",
    "        print(\"Training Loss:\",np.mean(training_loss_batch))\n",
    "        print(\"Validation Loss:\",np.mean(validation_loss_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210\n",
      "Training Loss: 0.00039801248281330507\n",
      "Validation Loss: 0.0003534686591004378\n",
      "220\n",
      "Training Loss: 0.00039836232849250184\n",
      "Validation Loss: 0.00035328564266683186\n",
      "230\n",
      "Training Loss: 0.00039792265793551487\n",
      "Validation Loss: 0.0003534061486454756\n"
     ]
    }
   ],
   "source": [
    "optimizer1 = optim.Adam(model1.parameters(), lr=0.0001)\n",
    "\n",
    "for epoch in range(210,240):\n",
    "    training_loss_batch=[]\n",
    "    validation_loss_batch=[]\n",
    "    for X_train, y_train in train_loader1:\n",
    "        model1.train()\n",
    "        optimizer1.zero_grad()\n",
    "        output,features = model1(X_train)\n",
    "        loss1 = criterion1(output, y_train.reshape(-1,1))\n",
    "        loss1.backward()\n",
    "        optimizer1.step()\n",
    "        training_loss_batch.append(loss1.item())\n",
    "        model1.eval()\n",
    "        validation_loss_batch.append(criterion1(model1(X_test1)[0],y_test1.reshape(-1,1)).item())\n",
    "    validation_loss_model1.append(validation_loss_batch)\n",
    "    training_loss_model1.append(training_loss_batch)\n",
    "    if epoch%10==0:\n",
    "        print(epoch)\n",
    "        print(\"Training Loss:\",np.mean(training_loss_batch))\n",
    "        print(\"Validation Loss:\",np.mean(validation_loss_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model1.state_dict(), \"stores_model_final.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
